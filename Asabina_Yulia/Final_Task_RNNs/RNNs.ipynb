{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpUwPKrvMA9X",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1b37e32d-95fa-48e3-c7e3-3de2d2923506"
      },
      "source": [
        "!pip3 -qq install torch==1.4.0\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 16.0MB 198kB/s \n",
            "\u001b[?25h  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 3.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6d627548-952f-4961-9685-535dbf205ac6"
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "6abe615c-867d-4a08-e9f3-12d2e4ea49c2"
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5cc7d5d0-0691-49fa-d565-a138e24b25b1"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cbe6b01-c2d5-4e5a-bfd8-945c49b566b0"
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'NUM', 'ADV', 'ADP', 'X', 'NOUN', '.', 'CONJ', 'PRON', 'ADJ', 'PRT', 'VERB', 'DET'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "8080cd64-495d-4994-f3ed-8ed63e9598ec"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAEvCAYAAAAemFY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad5dbbd6-8643-499f-f44a-8796d60273c5"
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24c19e3b-ecbe-4788-bfae-08184d21367e"
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "293e0e10-8bfa-4699-e2f8-7f3a68d3cff1"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45e2afcd-ef04-4104-fccf-99af062f708a"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self.output_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding = self.embedding(inputs)\n",
        "        out, _ = self.lstm(embedding)\n",
        "        return self.output_layer(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "colab": {}
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "def calc_accuracy(logits, y_batch):\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    correct = ((predictions == y_batch).float() * (y_batch != 0).float()).sum().item()\n",
        "    total = (y_batch != 0).float().sum().item()\n",
        "    return correct, total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30e8b6e1-f44b-4ea2-ba53-b8574f7c7ec8"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1)).item()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.559807777404785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "                logits_reshp = logits.view(-1, logits.shape[-1])\n",
        "\n",
        "                loss = criterion(logits_reshp, y_batch.view(-1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                cur_correct_count, cur_sum_count = calc_accuracy(logits, y_batch)\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0139c7ce-7ca8-47ae-f107-4a5fd694417b"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.31182, Accuracy = 72.29%: 100%|██████████| 572/572 [00:06<00:00, 90.92it/s]\n",
            "[1 / 50]   Val: Loss = 0.10366, Accuracy = 85.11%: 100%|██████████| 13/13 [00:00<00:00, 71.61it/s]\n",
            "[2 / 50] Train: Loss = 0.09943, Accuracy = 90.07%: 100%|██████████| 572/572 [00:06<00:00, 94.32it/s]\n",
            "[2 / 50]   Val: Loss = 0.07502, Accuracy = 89.53%: 100%|██████████| 13/13 [00:00<00:00, 75.61it/s]\n",
            "[3 / 50] Train: Loss = 0.06716, Accuracy = 93.25%: 100%|██████████| 572/572 [00:05<00:00, 95.64it/s]\n",
            "[3 / 50]   Val: Loss = 0.06636, Accuracy = 91.23%: 100%|██████████| 13/13 [00:00<00:00, 74.78it/s]\n",
            "[4 / 50] Train: Loss = 0.05039, Accuracy = 94.85%: 100%|██████████| 572/572 [00:06<00:00, 95.19it/s]\n",
            "[4 / 50]   Val: Loss = 0.06374, Accuracy = 92.09%: 100%|██████████| 13/13 [00:00<00:00, 79.58it/s]\n",
            "[5 / 50] Train: Loss = 0.04058, Accuracy = 95.83%: 100%|██████████| 572/572 [00:05<00:00, 96.26it/s]\n",
            "[5 / 50]   Val: Loss = 0.06247, Accuracy = 92.66%: 100%|██████████| 13/13 [00:00<00:00, 76.41it/s]\n",
            "[6 / 50] Train: Loss = 0.03315, Accuracy = 96.57%: 100%|██████████| 572/572 [00:05<00:00, 95.84it/s]\n",
            "[6 / 50]   Val: Loss = 0.05967, Accuracy = 92.98%: 100%|██████████| 13/13 [00:00<00:00, 77.78it/s]\n",
            "[7 / 50] Train: Loss = 0.02764, Accuracy = 97.14%: 100%|██████████| 572/572 [00:06<00:00, 94.85it/s]\n",
            "[7 / 50]   Val: Loss = 0.06093, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 77.11it/s]\n",
            "[8 / 50] Train: Loss = 0.02292, Accuracy = 97.61%: 100%|██████████| 572/572 [00:05<00:00, 95.70it/s]\n",
            "[8 / 50]   Val: Loss = 0.06277, Accuracy = 93.21%: 100%|██████████| 13/13 [00:00<00:00, 73.55it/s]\n",
            "[9 / 50] Train: Loss = 0.01914, Accuracy = 98.00%: 100%|██████████| 572/572 [00:06<00:00, 94.28it/s]\n",
            "[9 / 50]   Val: Loss = 0.07035, Accuracy = 93.34%: 100%|██████████| 13/13 [00:00<00:00, 77.92it/s]\n",
            "[10 / 50] Train: Loss = 0.01612, Accuracy = 98.32%: 100%|██████████| 572/572 [00:05<00:00, 98.27it/s]\n",
            "[10 / 50]   Val: Loss = 0.07481, Accuracy = 93.32%: 100%|██████████| 13/13 [00:00<00:00, 78.27it/s]\n",
            "[11 / 50] Train: Loss = 0.01341, Accuracy = 98.61%: 100%|██████████| 572/572 [00:05<00:00, 97.02it/s]\n",
            "[11 / 50]   Val: Loss = 0.06871, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 75.89it/s]\n",
            "[12 / 50] Train: Loss = 0.01114, Accuracy = 98.88%: 100%|██████████| 572/572 [00:05<00:00, 95.83it/s]\n",
            "[12 / 50]   Val: Loss = 0.07823, Accuracy = 93.19%: 100%|██████████| 13/13 [00:00<00:00, 76.88it/s]\n",
            "[13 / 50] Train: Loss = 0.00912, Accuracy = 99.08%: 100%|██████████| 572/572 [00:06<00:00, 94.95it/s]\n",
            "[13 / 50]   Val: Loss = 0.07774, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 72.74it/s]\n",
            "[14 / 50] Train: Loss = 0.00748, Accuracy = 99.27%: 100%|██████████| 572/572 [00:05<00:00, 97.15it/s]\n",
            "[14 / 50]   Val: Loss = 0.08051, Accuracy = 92.95%: 100%|██████████| 13/13 [00:00<00:00, 74.91it/s]\n",
            "[15 / 50] Train: Loss = 0.00611, Accuracy = 99.41%: 100%|██████████| 572/572 [00:05<00:00, 97.88it/s]\n",
            "[15 / 50]   Val: Loss = 0.08984, Accuracy = 92.99%: 100%|██████████| 13/13 [00:00<00:00, 76.16it/s]\n",
            "[16 / 50] Train: Loss = 0.00496, Accuracy = 99.54%: 100%|██████████| 572/572 [00:05<00:00, 98.43it/s]\n",
            "[16 / 50]   Val: Loss = 0.09002, Accuracy = 92.96%: 100%|██████████| 13/13 [00:00<00:00, 70.39it/s]\n",
            "[17 / 50] Train: Loss = 0.00412, Accuracy = 99.63%: 100%|██████████| 572/572 [00:06<00:00, 95.11it/s]\n",
            "[17 / 50]   Val: Loss = 0.09364, Accuracy = 92.85%: 100%|██████████| 13/13 [00:00<00:00, 74.49it/s]\n",
            "[18 / 50] Train: Loss = 0.00346, Accuracy = 99.69%: 100%|██████████| 572/572 [00:05<00:00, 95.59it/s]\n",
            "[18 / 50]   Val: Loss = 0.10627, Accuracy = 92.90%: 100%|██████████| 13/13 [00:00<00:00, 73.86it/s]\n",
            "[19 / 50] Train: Loss = 0.00286, Accuracy = 99.74%: 100%|██████████| 572/572 [00:05<00:00, 97.20it/s] \n",
            "[19 / 50]   Val: Loss = 0.10493, Accuracy = 92.89%: 100%|██████████| 13/13 [00:00<00:00, 76.23it/s]\n",
            "[20 / 50] Train: Loss = 0.00248, Accuracy = 99.78%: 100%|██████████| 572/572 [00:05<00:00, 97.24it/s]\n",
            "[20 / 50]   Val: Loss = 0.10917, Accuracy = 92.91%: 100%|██████████| 13/13 [00:00<00:00, 77.67it/s]\n",
            "[21 / 50] Train: Loss = 0.00226, Accuracy = 99.79%: 100%|██████████| 572/572 [00:05<00:00, 98.83it/s]\n",
            "[21 / 50]   Val: Loss = 0.10817, Accuracy = 92.79%: 100%|██████████| 13/13 [00:00<00:00, 71.86it/s]\n",
            "[22 / 50] Train: Loss = 0.00215, Accuracy = 99.79%: 100%|██████████| 572/572 [00:05<00:00, 95.70it/s]\n",
            "[22 / 50]   Val: Loss = 0.11945, Accuracy = 92.91%: 100%|██████████| 13/13 [00:00<00:00, 75.59it/s]\n",
            "[23 / 50] Train: Loss = 0.00209, Accuracy = 99.80%: 100%|██████████| 572/572 [00:06<00:00, 94.99it/s]\n",
            "[23 / 50]   Val: Loss = 0.11650, Accuracy = 92.82%: 100%|██████████| 13/13 [00:00<00:00, 76.20it/s]\n",
            "[24 / 50] Train: Loss = 0.00187, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 98.53it/s] \n",
            "[24 / 50]   Val: Loss = 0.12733, Accuracy = 92.96%: 100%|██████████| 13/13 [00:00<00:00, 75.65it/s]\n",
            "[25 / 50] Train: Loss = 0.00188, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 97.39it/s]\n",
            "[25 / 50]   Val: Loss = 0.11769, Accuracy = 92.91%: 100%|██████████| 13/13 [00:00<00:00, 76.54it/s]\n",
            "[26 / 50] Train: Loss = 0.00174, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 96.43it/s]\n",
            "[26 / 50]   Val: Loss = 0.12285, Accuracy = 92.87%: 100%|██████████| 13/13 [00:00<00:00, 71.34it/s]\n",
            "[27 / 50] Train: Loss = 0.00176, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 96.70it/s]\n",
            "[27 / 50]   Val: Loss = 0.13130, Accuracy = 92.83%: 100%|██████████| 13/13 [00:00<00:00, 76.18it/s]\n",
            "[28 / 50] Train: Loss = 0.00179, Accuracy = 99.81%: 100%|██████████| 572/572 [00:06<00:00, 94.62it/s]\n",
            "[28 / 50]   Val: Loss = 0.12256, Accuracy = 92.89%: 100%|██████████| 13/13 [00:00<00:00, 73.99it/s]\n",
            "[29 / 50] Train: Loss = 0.00155, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 96.89it/s]\n",
            "[29 / 50]   Val: Loss = 0.13307, Accuracy = 92.82%: 100%|██████████| 13/13 [00:00<00:00, 75.30it/s]\n",
            "[30 / 50] Train: Loss = 0.00162, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 97.59it/s]\n",
            "[30 / 50]   Val: Loss = 0.13364, Accuracy = 92.87%: 100%|██████████| 13/13 [00:00<00:00, 76.58it/s]\n",
            "[31 / 50] Train: Loss = 0.00174, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 96.25it/s]\n",
            "[31 / 50]   Val: Loss = 0.13529, Accuracy = 92.94%: 100%|██████████| 13/13 [00:00<00:00, 74.47it/s]\n",
            "[32 / 50] Train: Loss = 0.00169, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 95.81it/s]\n",
            "[32 / 50]   Val: Loss = 0.13381, Accuracy = 92.88%: 100%|██████████| 13/13 [00:00<00:00, 76.52it/s]\n",
            "[33 / 50] Train: Loss = 0.00152, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 96.19it/s]\n",
            "[33 / 50]   Val: Loss = 0.14075, Accuracy = 92.93%: 100%|██████████| 13/13 [00:00<00:00, 76.29it/s]\n",
            "[34 / 50] Train: Loss = 0.00160, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 96.58it/s]\n",
            "[34 / 50]   Val: Loss = 0.14780, Accuracy = 92.83%: 100%|██████████| 13/13 [00:00<00:00, 80.64it/s]\n",
            "[35 / 50] Train: Loss = 0.00156, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 97.29it/s]\n",
            "[35 / 50]   Val: Loss = 0.13486, Accuracy = 92.91%: 100%|██████████| 13/13 [00:00<00:00, 73.24it/s]\n",
            "[36 / 50] Train: Loss = 0.00147, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 97.47it/s]\n",
            "[36 / 50]   Val: Loss = 0.15356, Accuracy = 92.93%: 100%|██████████| 13/13 [00:00<00:00, 72.89it/s]\n",
            "[37 / 50] Train: Loss = 0.00155, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 95.30it/s]\n",
            "[37 / 50]   Val: Loss = 0.15030, Accuracy = 92.92%: 100%|██████████| 13/13 [00:00<00:00, 74.97it/s]\n",
            "[38 / 50] Train: Loss = 0.00152, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 93.60it/s]\n",
            "[38 / 50]   Val: Loss = 0.14022, Accuracy = 92.89%: 100%|██████████| 13/13 [00:00<00:00, 71.85it/s]\n",
            "[39 / 50] Train: Loss = 0.00152, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 97.84it/s]\n",
            "[39 / 50]   Val: Loss = 0.16224, Accuracy = 92.88%: 100%|██████████| 13/13 [00:00<00:00, 78.96it/s]\n",
            "[40 / 50] Train: Loss = 0.00135, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 96.09it/s]\n",
            "[40 / 50]   Val: Loss = 0.15673, Accuracy = 92.98%: 100%|██████████| 13/13 [00:00<00:00, 77.84it/s]\n",
            "[41 / 50] Train: Loss = 0.00131, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 94.92it/s]\n",
            "[41 / 50]   Val: Loss = 0.15209, Accuracy = 92.96%: 100%|██████████| 13/13 [00:00<00:00, 72.69it/s]\n",
            "[42 / 50] Train: Loss = 0.00136, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 94.77it/s]\n",
            "[42 / 50]   Val: Loss = 0.16917, Accuracy = 92.99%: 100%|██████████| 13/13 [00:00<00:00, 76.49it/s]\n",
            "[43 / 50] Train: Loss = 0.00163, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 97.10it/s]\n",
            "[43 / 50]   Val: Loss = 0.16006, Accuracy = 92.91%: 100%|██████████| 13/13 [00:00<00:00, 75.36it/s]\n",
            "[44 / 50] Train: Loss = 0.00168, Accuracy = 99.80%: 100%|██████████| 572/572 [00:05<00:00, 97.27it/s]\n",
            "[44 / 50]   Val: Loss = 0.16066, Accuracy = 92.94%: 100%|██████████| 13/13 [00:00<00:00, 73.53it/s]\n",
            "[45 / 50] Train: Loss = 0.00150, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 97.79it/s] \n",
            "[45 / 50]   Val: Loss = 0.15666, Accuracy = 92.97%: 100%|██████████| 13/13 [00:00<00:00, 76.91it/s]\n",
            "[46 / 50] Train: Loss = 0.00130, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 95.69it/s]\n",
            "[46 / 50]   Val: Loss = 0.16199, Accuracy = 92.92%: 100%|██████████| 13/13 [00:00<00:00, 74.13it/s]\n",
            "[47 / 50] Train: Loss = 0.00128, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 95.70it/s]\n",
            "[47 / 50]   Val: Loss = 0.15956, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 70.45it/s]\n",
            "[48 / 50] Train: Loss = 0.00127, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 96.22it/s]\n",
            "[48 / 50]   Val: Loss = 0.16403, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 74.11it/s]\n",
            "[49 / 50] Train: Loss = 0.00128, Accuracy = 99.85%: 100%|██████████| 572/572 [00:05<00:00, 96.84it/s]\n",
            "[49 / 50]   Val: Loss = 0.16388, Accuracy = 93.02%: 100%|██████████| 13/13 [00:00<00:00, 74.85it/s]\n",
            "[50 / 50] Train: Loss = 0.00128, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 98.81it/s] \n",
            "[50 / 50]   Val: Loss = 0.16906, Accuracy = 93.06%: 100%|██████████| 13/13 [00:00<00:00, 75.47it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98wr38_rw55D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67f1df59-1af1-466a-9710-effc14f8d208"
      },
      "source": [
        "correct, total = 0, 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    cur_correct, cur_total = calc_accuracy(logits, y_batch)\n",
        "    correct += cur_correct\n",
        "    total += cur_total\n",
        "\n",
        "print(correct/total)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9308704199071157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCQ0BjffMA-g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a636bad8-1733-406d-cfae-e0a79b5e26f5"
      },
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
        "        self.output_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding = self.embedding(inputs)\n",
        "        out, _ = self.lstm(embedding)\n",
        "        return self.output_layer(out)\n",
        "\n",
        "model = BidirectionalLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.25365, Accuracy = 76.76%: 100%|██████████| 572/572 [00:07<00:00, 77.89it/s]\n",
            "[1 / 50]   Val: Loss = 0.07091, Accuracy = 89.60%: 100%|██████████| 13/13 [00:00<00:00, 64.13it/s]\n",
            "[2 / 50] Train: Loss = 0.07614, Accuracy = 92.66%: 100%|██████████| 572/572 [00:07<00:00, 78.75it/s]\n",
            "[2 / 50]   Val: Loss = 0.04595, Accuracy = 93.57%: 100%|██████████| 13/13 [00:00<00:00, 64.38it/s]\n",
            "[3 / 50] Train: Loss = 0.04865, Accuracy = 95.39%: 100%|██████████| 572/572 [00:07<00:00, 78.92it/s]\n",
            "[3 / 50]   Val: Loss = 0.03309, Accuracy = 94.95%: 100%|██████████| 13/13 [00:00<00:00, 56.62it/s]\n",
            "[4 / 50] Train: Loss = 0.03420, Accuracy = 96.78%: 100%|██████████| 572/572 [00:07<00:00, 79.27it/s]\n",
            "[4 / 50]   Val: Loss = 0.03220, Accuracy = 95.63%: 100%|██████████| 13/13 [00:00<00:00, 65.50it/s]\n",
            "[5 / 50] Train: Loss = 0.02468, Accuracy = 97.68%: 100%|██████████| 572/572 [00:07<00:00, 78.95it/s]\n",
            "[5 / 50]   Val: Loss = 0.02762, Accuracy = 95.89%: 100%|██████████| 13/13 [00:00<00:00, 62.92it/s]\n",
            "[6 / 50] Train: Loss = 0.01821, Accuracy = 98.35%: 100%|██████████| 572/572 [00:07<00:00, 78.88it/s]\n",
            "[6 / 50]   Val: Loss = 0.02991, Accuracy = 96.00%: 100%|██████████| 13/13 [00:00<00:00, 64.77it/s]\n",
            "[7 / 50] Train: Loss = 0.01284, Accuracy = 98.83%: 100%|██████████| 572/572 [00:07<00:00, 77.31it/s]\n",
            "[7 / 50]   Val: Loss = 0.02980, Accuracy = 96.12%: 100%|██████████| 13/13 [00:00<00:00, 64.40it/s]\n",
            "[8 / 50] Train: Loss = 0.00898, Accuracy = 99.23%: 100%|██████████| 572/572 [00:07<00:00, 78.84it/s]\n",
            "[8 / 50]   Val: Loss = 0.03078, Accuracy = 96.14%: 100%|██████████| 13/13 [00:00<00:00, 63.06it/s]\n",
            "[9 / 50] Train: Loss = 0.00614, Accuracy = 99.50%: 100%|██████████| 572/572 [00:07<00:00, 80.33it/s]\n",
            "[9 / 50]   Val: Loss = 0.02994, Accuracy = 96.23%: 100%|██████████| 13/13 [00:00<00:00, 62.22it/s]\n",
            "[10 / 50] Train: Loss = 0.00392, Accuracy = 99.71%: 100%|██████████| 572/572 [00:07<00:00, 79.25it/s]\n",
            "[10 / 50]   Val: Loss = 0.03331, Accuracy = 96.03%: 100%|██████████| 13/13 [00:00<00:00, 62.52it/s]\n",
            "[11 / 50] Train: Loss = 0.00247, Accuracy = 99.84%: 100%|██████████| 572/572 [00:07<00:00, 78.62it/s]\n",
            "[11 / 50]   Val: Loss = 0.03389, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 59.13it/s]\n",
            "[12 / 50] Train: Loss = 0.00153, Accuracy = 99.92%: 100%|██████████| 572/572 [00:07<00:00, 79.61it/s]\n",
            "[12 / 50]   Val: Loss = 0.03567, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 61.58it/s]\n",
            "[13 / 50] Train: Loss = 0.00090, Accuracy = 99.96%: 100%|██████████| 572/572 [00:07<00:00, 80.11it/s]\n",
            "[13 / 50]   Val: Loss = 0.04284, Accuracy = 96.26%: 100%|██████████| 13/13 [00:00<00:00, 67.32it/s]\n",
            "[14 / 50] Train: Loss = 0.00058, Accuracy = 99.98%: 100%|██████████| 572/572 [00:07<00:00, 79.45it/s]\n",
            "[14 / 50]   Val: Loss = 0.04049, Accuracy = 96.21%: 100%|██████████| 13/13 [00:00<00:00, 64.57it/s]\n",
            "[15 / 50] Train: Loss = 0.00035, Accuracy = 99.99%: 100%|██████████| 572/572 [00:07<00:00, 78.06it/s]\n",
            "[15 / 50]   Val: Loss = 0.04020, Accuracy = 96.23%: 100%|██████████| 13/13 [00:00<00:00, 62.38it/s]\n",
            "[16 / 50] Train: Loss = 0.00070, Accuracy = 99.95%: 100%|██████████| 572/572 [00:07<00:00, 79.28it/s]\n",
            "[16 / 50]   Val: Loss = 0.04448, Accuracy = 95.97%: 100%|██████████| 13/13 [00:00<00:00, 63.16it/s]\n",
            "[17 / 50] Train: Loss = 0.00129, Accuracy = 99.88%: 100%|██████████| 572/572 [00:07<00:00, 78.97it/s]\n",
            "[17 / 50]   Val: Loss = 0.04509, Accuracy = 96.10%: 100%|██████████| 13/13 [00:00<00:00, 62.57it/s]\n",
            "[18 / 50] Train: Loss = 0.00044, Accuracy = 99.98%: 100%|██████████| 572/572 [00:07<00:00, 78.69it/s]\n",
            "[18 / 50]   Val: Loss = 0.04992, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 64.14it/s]\n",
            "[19 / 50] Train: Loss = 0.00020, Accuracy = 99.99%: 100%|██████████| 572/572 [00:07<00:00, 77.86it/s]\n",
            "[19 / 50]   Val: Loss = 0.04673, Accuracy = 96.20%: 100%|██████████| 13/13 [00:00<00:00, 63.19it/s]\n",
            "[20 / 50] Train: Loss = 0.00008, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 78.40it/s]\n",
            "[20 / 50]   Val: Loss = 0.04993, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 65.94it/s]\n",
            "[21 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 79.09it/s]\n",
            "[21 / 50]   Val: Loss = 0.04777, Accuracy = 96.23%: 100%|██████████| 13/13 [00:00<00:00, 65.14it/s]\n",
            "[22 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 78.61it/s]\n",
            "[22 / 50]   Val: Loss = 0.05153, Accuracy = 96.17%: 100%|██████████| 13/13 [00:00<00:00, 63.05it/s]\n",
            "[23 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 79.36it/s]\n",
            "[23 / 50]   Val: Loss = 0.04749, Accuracy = 96.09%: 100%|██████████| 13/13 [00:00<00:00, 61.78it/s]\n",
            "[24 / 50] Train: Loss = 0.00210, Accuracy = 99.78%: 100%|██████████| 572/572 [00:07<00:00, 78.68it/s]\n",
            "[24 / 50]   Val: Loss = 0.05311, Accuracy = 95.79%: 100%|██████████| 13/13 [00:00<00:00, 63.89it/s]\n",
            "[25 / 50] Train: Loss = 0.00075, Accuracy = 99.93%: 100%|██████████| 572/572 [00:07<00:00, 79.76it/s]\n",
            "[25 / 50]   Val: Loss = 0.05105, Accuracy = 96.00%: 100%|██████████| 13/13 [00:00<00:00, 64.36it/s]\n",
            "[26 / 50] Train: Loss = 0.00012, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 78.47it/s]\n",
            "[26 / 50]   Val: Loss = 0.05262, Accuracy = 96.05%: 100%|██████████| 13/13 [00:00<00:00, 64.75it/s]\n",
            "[27 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 78.02it/s]\n",
            "[27 / 50]   Val: Loss = 0.05222, Accuracy = 96.05%: 100%|██████████| 13/13 [00:00<00:00, 63.59it/s]\n",
            "[28 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 79.53it/s]\n",
            "[28 / 50]   Val: Loss = 0.05403, Accuracy = 96.05%: 100%|██████████| 13/13 [00:00<00:00, 65.04it/s]\n",
            "[29 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 80.08it/s]\n",
            "[29 / 50]   Val: Loss = 0.05146, Accuracy = 96.03%: 100%|██████████| 13/13 [00:00<00:00, 59.45it/s]\n",
            "[30 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 79.43it/s]\n",
            "[30 / 50]   Val: Loss = 0.05269, Accuracy = 96.03%: 100%|██████████| 13/13 [00:00<00:00, 61.12it/s]\n",
            "[31 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 78.12it/s]\n",
            "[31 / 50]   Val: Loss = 0.05720, Accuracy = 96.01%: 100%|██████████| 13/13 [00:00<00:00, 61.95it/s]\n",
            "[32 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 78.54it/s]\n",
            "[32 / 50]   Val: Loss = 0.05979, Accuracy = 95.93%: 100%|██████████| 13/13 [00:00<00:00, 67.25it/s]\n",
            "[33 / 50] Train: Loss = 0.00188, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 80.99it/s]\n",
            "[33 / 50]   Val: Loss = 0.05588, Accuracy = 95.90%: 100%|██████████| 13/13 [00:00<00:00, 62.29it/s]\n",
            "[34 / 50] Train: Loss = 0.00036, Accuracy = 99.97%: 100%|██████████| 572/572 [00:07<00:00, 78.66it/s]\n",
            "[34 / 50]   Val: Loss = 0.05586, Accuracy = 96.15%: 100%|██████████| 13/13 [00:00<00:00, 64.50it/s]\n",
            "[35 / 50] Train: Loss = 0.00010, Accuracy = 99.99%: 100%|██████████| 572/572 [00:07<00:00, 78.29it/s]\n",
            "[35 / 50]   Val: Loss = 0.05787, Accuracy = 96.20%: 100%|██████████| 13/13 [00:00<00:00, 64.03it/s]\n",
            "[36 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 78.41it/s]\n",
            "[36 / 50]   Val: Loss = 0.05303, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 62.46it/s]\n",
            "[37 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 79.65it/s]\n",
            "[37 / 50]   Val: Loss = 0.05300, Accuracy = 96.23%: 100%|██████████| 13/13 [00:00<00:00, 63.29it/s]\n",
            "[38 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 78.13it/s]\n",
            "[38 / 50]   Val: Loss = 0.05610, Accuracy = 96.23%: 100%|██████████| 13/13 [00:00<00:00, 65.46it/s]\n",
            "[39 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 77.47it/s]\n",
            "[39 / 50]   Val: Loss = 0.05862, Accuracy = 96.24%: 100%|██████████| 13/13 [00:00<00:00, 64.06it/s]\n",
            "[40 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 76.68it/s]\n",
            "[40 / 50]   Val: Loss = 0.05610, Accuracy = 96.24%: 100%|██████████| 13/13 [00:00<00:00, 64.22it/s]\n",
            "[41 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 79.60it/s]\n",
            "[41 / 50]   Val: Loss = 0.05577, Accuracy = 96.24%: 100%|██████████| 13/13 [00:00<00:00, 62.04it/s]\n",
            "[42 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 78.53it/s]\n",
            "[42 / 50]   Val: Loss = 0.05699, Accuracy = 96.17%: 100%|██████████| 13/13 [00:00<00:00, 62.82it/s]\n",
            "[43 / 50] Train: Loss = 0.00189, Accuracy = 99.81%: 100%|██████████| 572/572 [00:07<00:00, 78.90it/s]\n",
            "[43 / 50]   Val: Loss = 0.05672, Accuracy = 96.13%: 100%|██████████| 13/13 [00:00<00:00, 63.44it/s]\n",
            "[44 / 50] Train: Loss = 0.00033, Accuracy = 99.97%: 100%|██████████| 572/572 [00:07<00:00, 78.36it/s]\n",
            "[44 / 50]   Val: Loss = 0.05503, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 62.34it/s]\n",
            "[45 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 79.28it/s]\n",
            "[45 / 50]   Val: Loss = 0.05682, Accuracy = 96.29%: 100%|██████████| 13/13 [00:00<00:00, 63.72it/s]\n",
            "[46 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 79.33it/s]\n",
            "[46 / 50]   Val: Loss = 0.05931, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 64.84it/s]\n",
            "[47 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 78.36it/s]\n",
            "[47 / 50]   Val: Loss = 0.05840, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 65.15it/s]\n",
            "[48 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 78.14it/s]\n",
            "[48 / 50]   Val: Loss = 0.05831, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 63.22it/s]\n",
            "[49 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 79.90it/s]\n",
            "[49 / 50]   Val: Loss = 0.05984, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 65.41it/s]\n",
            "[50 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 80.00it/s]\n",
            "[50 / 50]   Val: Loss = 0.06291, Accuracy = 96.30%: 100%|██████████| 13/13 [00:00<00:00, 62.76it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0258cb78-f58d-48b4-a107-a68bbaab2263"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "310f5d65-a5a7-410d-df7d-ec958c87b06a"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
        "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self.output_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding = self.embedding(inputs)\n",
        "        out, _ = self.lstm(embedding)\n",
        "        return self.output_layer(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a19b09e-ba3d-4fba-df3e-5302416de66c"
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=torch.FloatTensor(embeddings),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.77408, Accuracy = 77.53%: 100%|██████████| 572/572 [00:05<00:00, 108.26it/s]\n",
            "[1 / 50]   Val: Loss = 0.38655, Accuracy = 88.68%: 100%|██████████| 13/13 [00:00<00:00, 88.24it/s]\n",
            "[2 / 50] Train: Loss = 0.29560, Accuracy = 91.16%: 100%|██████████| 572/572 [00:05<00:00, 110.25it/s]\n",
            "[2 / 50]   Val: Loss = 0.26515, Accuracy = 91.91%: 100%|██████████| 13/13 [00:00<00:00, 87.48it/s]\n",
            "[3 / 50] Train: Loss = 0.21650, Accuracy = 93.26%: 100%|██████████| 572/572 [00:05<00:00, 111.41it/s]\n",
            "[3 / 50]   Val: Loss = 0.21550, Accuracy = 93.23%: 100%|██████████| 13/13 [00:00<00:00, 87.56it/s]\n",
            "[4 / 50] Train: Loss = 0.17793, Accuracy = 94.35%: 100%|██████████| 572/572 [00:05<00:00, 111.67it/s]\n",
            "[4 / 50]   Val: Loss = 0.18962, Accuracy = 94.01%: 100%|██████████| 13/13 [00:00<00:00, 88.92it/s]\n",
            "[5 / 50] Train: Loss = 0.15633, Accuracy = 94.96%: 100%|██████████| 572/572 [00:05<00:00, 111.38it/s]\n",
            "[5 / 50]   Val: Loss = 0.17407, Accuracy = 94.40%: 100%|██████████| 13/13 [00:00<00:00, 90.95it/s]\n",
            "[6 / 50] Train: Loss = 0.14153, Accuracy = 95.35%: 100%|██████████| 572/572 [00:05<00:00, 111.35it/s]\n",
            "[6 / 50]   Val: Loss = 0.16334, Accuracy = 94.65%: 100%|██████████| 13/13 [00:00<00:00, 86.29it/s]\n",
            "[7 / 50] Train: Loss = 0.13124, Accuracy = 95.62%: 100%|██████████| 572/572 [00:04<00:00, 114.55it/s]\n",
            "[7 / 50]   Val: Loss = 0.15635, Accuracy = 94.84%: 100%|██████████| 13/13 [00:00<00:00, 86.65it/s]\n",
            "[8 / 50] Train: Loss = 0.12358, Accuracy = 95.84%: 100%|██████████| 572/572 [00:04<00:00, 115.20it/s]\n",
            "[8 / 50]   Val: Loss = 0.15168, Accuracy = 94.99%: 100%|██████████| 13/13 [00:00<00:00, 79.24it/s]\n",
            "[9 / 50] Train: Loss = 0.11754, Accuracy = 96.00%: 100%|██████████| 572/572 [00:05<00:00, 111.76it/s]\n",
            "[9 / 50]   Val: Loss = 0.14638, Accuracy = 95.11%: 100%|██████████| 13/13 [00:00<00:00, 84.37it/s]\n",
            "[10 / 50] Train: Loss = 0.11268, Accuracy = 96.13%: 100%|██████████| 572/572 [00:05<00:00, 110.57it/s]\n",
            "[10 / 50]   Val: Loss = 0.14349, Accuracy = 95.20%: 100%|██████████| 13/13 [00:00<00:00, 86.71it/s]\n",
            "[11 / 50] Train: Loss = 0.10869, Accuracy = 96.27%: 100%|██████████| 572/572 [00:05<00:00, 110.98it/s]\n",
            "[11 / 50]   Val: Loss = 0.14407, Accuracy = 95.10%: 100%|██████████| 13/13 [00:00<00:00, 84.47it/s]\n",
            "[12 / 50] Train: Loss = 0.10540, Accuracy = 96.36%: 100%|██████████| 572/572 [00:05<00:00, 108.71it/s]\n",
            "[12 / 50]   Val: Loss = 0.13959, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 92.00it/s]\n",
            "[13 / 50] Train: Loss = 0.10234, Accuracy = 96.46%: 100%|██████████| 572/572 [00:05<00:00, 113.49it/s]\n",
            "[13 / 50]   Val: Loss = 0.13858, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 86.69it/s]\n",
            "[14 / 50] Train: Loss = 0.09982, Accuracy = 96.54%: 100%|██████████| 572/572 [00:04<00:00, 114.74it/s]\n",
            "[14 / 50]   Val: Loss = 0.13633, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 89.52it/s]\n",
            "[15 / 50] Train: Loss = 0.09739, Accuracy = 96.61%: 100%|██████████| 572/572 [00:04<00:00, 115.77it/s]\n",
            "[15 / 50]   Val: Loss = 0.13756, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 86.77it/s]\n",
            "[16 / 50] Train: Loss = 0.09527, Accuracy = 96.68%: 100%|██████████| 572/572 [00:05<00:00, 112.88it/s]\n",
            "[16 / 50]   Val: Loss = 0.13814, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 86.48it/s]\n",
            "[17 / 50] Train: Loss = 0.09343, Accuracy = 96.72%: 100%|██████████| 572/572 [00:05<00:00, 111.71it/s]\n",
            "[17 / 50]   Val: Loss = 0.13582, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 85.99it/s]\n",
            "[18 / 50] Train: Loss = 0.09162, Accuracy = 96.79%: 100%|██████████| 572/572 [00:05<00:00, 111.05it/s]\n",
            "[18 / 50]   Val: Loss = 0.13588, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 96.74it/s]\n",
            "[19 / 50] Train: Loss = 0.08984, Accuracy = 96.83%: 100%|██████████| 572/572 [00:04<00:00, 115.84it/s]\n",
            "[19 / 50]   Val: Loss = 0.13607, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 93.85it/s]\n",
            "[20 / 50] Train: Loss = 0.08842, Accuracy = 96.89%: 100%|██████████| 572/572 [00:04<00:00, 116.06it/s]\n",
            "[20 / 50]   Val: Loss = 0.13404, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 90.89it/s]\n",
            "[21 / 50] Train: Loss = 0.08698, Accuracy = 96.94%: 100%|██████████| 572/572 [00:05<00:00, 114.36it/s]\n",
            "[21 / 50]   Val: Loss = 0.13393, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 92.23it/s]\n",
            "[22 / 50] Train: Loss = 0.08565, Accuracy = 96.99%: 100%|██████████| 572/572 [00:05<00:00, 109.32it/s]\n",
            "[22 / 50]   Val: Loss = 0.13394, Accuracy = 95.56%: 100%|██████████| 13/13 [00:00<00:00, 91.91it/s]\n",
            "[23 / 50] Train: Loss = 0.08428, Accuracy = 97.03%: 100%|██████████| 572/572 [00:05<00:00, 109.11it/s]\n",
            "[23 / 50]   Val: Loss = 0.13332, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 87.51it/s]\n",
            "[24 / 50] Train: Loss = 0.08312, Accuracy = 97.08%: 100%|██████████| 572/572 [00:05<00:00, 111.54it/s]\n",
            "[24 / 50]   Val: Loss = 0.13463, Accuracy = 95.54%: 100%|██████████| 13/13 [00:00<00:00, 90.54it/s]\n",
            "[25 / 50] Train: Loss = 0.08192, Accuracy = 97.10%: 100%|██████████| 572/572 [00:05<00:00, 113.60it/s]\n",
            "[25 / 50]   Val: Loss = 0.13373, Accuracy = 95.44%: 100%|██████████| 13/13 [00:00<00:00, 93.90it/s]\n",
            "[26 / 50] Train: Loss = 0.08066, Accuracy = 97.14%: 100%|██████████| 572/572 [00:04<00:00, 115.44it/s]\n",
            "[26 / 50]   Val: Loss = 0.13370, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 92.05it/s]\n",
            "[27 / 50] Train: Loss = 0.07971, Accuracy = 97.17%: 100%|██████████| 572/572 [00:05<00:00, 111.67it/s]\n",
            "[27 / 50]   Val: Loss = 0.13401, Accuracy = 95.44%: 100%|██████████| 13/13 [00:00<00:00, 82.66it/s]\n",
            "[28 / 50] Train: Loss = 0.07872, Accuracy = 97.22%: 100%|██████████| 572/572 [00:05<00:00, 109.19it/s]\n",
            "[28 / 50]   Val: Loss = 0.13668, Accuracy = 95.53%: 100%|██████████| 13/13 [00:00<00:00, 83.53it/s]\n",
            "[29 / 50] Train: Loss = 0.07781, Accuracy = 97.22%: 100%|██████████| 572/572 [00:05<00:00, 113.25it/s]\n",
            "[29 / 50]   Val: Loss = 0.13461, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 92.48it/s]\n",
            "[30 / 50] Train: Loss = 0.07689, Accuracy = 97.27%: 100%|██████████| 572/572 [00:04<00:00, 114.75it/s]\n",
            "[30 / 50]   Val: Loss = 0.13402, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 89.61it/s]\n",
            "[31 / 50] Train: Loss = 0.07603, Accuracy = 97.30%: 100%|██████████| 572/572 [00:04<00:00, 114.44it/s]\n",
            "[31 / 50]   Val: Loss = 0.13487, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 85.40it/s]\n",
            "[32 / 50] Train: Loss = 0.07514, Accuracy = 97.33%: 100%|██████████| 572/572 [00:05<00:00, 113.63it/s]\n",
            "[32 / 50]   Val: Loss = 0.13457, Accuracy = 95.52%: 100%|██████████| 13/13 [00:00<00:00, 89.50it/s]\n",
            "[33 / 50] Train: Loss = 0.07413, Accuracy = 97.36%: 100%|██████████| 572/572 [00:05<00:00, 111.52it/s]\n",
            "[33 / 50]   Val: Loss = 0.13693, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 88.98it/s]\n",
            "[34 / 50] Train: Loss = 0.07343, Accuracy = 97.40%: 100%|██████████| 572/572 [00:05<00:00, 109.77it/s]\n",
            "[34 / 50]   Val: Loss = 0.13678, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 80.32it/s]\n",
            "[35 / 50] Train: Loss = 0.07253, Accuracy = 97.40%: 100%|██████████| 572/572 [00:05<00:00, 110.85it/s]\n",
            "[35 / 50]   Val: Loss = 0.13866, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 90.23it/s]\n",
            "[36 / 50] Train: Loss = 0.07201, Accuracy = 97.44%: 100%|██████████| 572/572 [00:04<00:00, 115.89it/s]\n",
            "[36 / 50]   Val: Loss = 0.13955, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 88.87it/s]\n",
            "[37 / 50] Train: Loss = 0.07128, Accuracy = 97.47%: 100%|██████████| 572/572 [00:04<00:00, 114.53it/s]\n",
            "[37 / 50]   Val: Loss = 0.13811, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 83.42it/s]\n",
            "[38 / 50] Train: Loss = 0.07047, Accuracy = 97.49%: 100%|██████████| 572/572 [00:04<00:00, 115.30it/s]\n",
            "[38 / 50]   Val: Loss = 0.13768, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 84.05it/s]\n",
            "[39 / 50] Train: Loss = 0.06988, Accuracy = 97.52%: 100%|██████████| 572/572 [00:05<00:00, 111.28it/s]\n",
            "[39 / 50]   Val: Loss = 0.13840, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 83.12it/s]\n",
            "[40 / 50] Train: Loss = 0.06923, Accuracy = 97.54%: 100%|██████████| 572/572 [00:05<00:00, 111.57it/s]\n",
            "[40 / 50]   Val: Loss = 0.13925, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 93.30it/s]\n",
            "[41 / 50] Train: Loss = 0.06852, Accuracy = 97.56%: 100%|██████████| 572/572 [00:05<00:00, 113.44it/s]\n",
            "[41 / 50]   Val: Loss = 0.14185, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 92.30it/s]\n",
            "[42 / 50] Train: Loss = 0.06797, Accuracy = 97.59%: 100%|██████████| 572/572 [00:05<00:00, 113.95it/s]\n",
            "[42 / 50]   Val: Loss = 0.14008, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 94.45it/s]\n",
            "[43 / 50] Train: Loss = 0.06739, Accuracy = 97.61%: 100%|██████████| 572/572 [00:05<00:00, 112.91it/s]\n",
            "[43 / 50]   Val: Loss = 0.13964, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 91.01it/s]\n",
            "[44 / 50] Train: Loss = 0.06665, Accuracy = 97.63%: 100%|██████████| 572/572 [00:05<00:00, 113.26it/s]\n",
            "[44 / 50]   Val: Loss = 0.14333, Accuracy = 95.30%: 100%|██████████| 13/13 [00:00<00:00, 87.47it/s]\n",
            "[45 / 50] Train: Loss = 0.06610, Accuracy = 97.67%: 100%|██████████| 572/572 [00:05<00:00, 110.22it/s]\n",
            "[45 / 50]   Val: Loss = 0.14242, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 87.81it/s]\n",
            "[46 / 50] Train: Loss = 0.06565, Accuracy = 97.66%: 100%|██████████| 572/572 [00:05<00:00, 107.83it/s]\n",
            "[46 / 50]   Val: Loss = 0.14425, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 86.72it/s]\n",
            "[47 / 50] Train: Loss = 0.06501, Accuracy = 97.67%: 100%|██████████| 572/572 [00:05<00:00, 113.20it/s]\n",
            "[47 / 50]   Val: Loss = 0.14403, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 88.21it/s]\n",
            "[48 / 50] Train: Loss = 0.06464, Accuracy = 97.69%: 100%|██████████| 572/572 [00:04<00:00, 114.89it/s]\n",
            "[48 / 50]   Val: Loss = 0.14437, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 92.95it/s]\n",
            "[49 / 50] Train: Loss = 0.06395, Accuracy = 97.72%: 100%|██████████| 572/572 [00:04<00:00, 114.41it/s]\n",
            "[49 / 50]   Val: Loss = 0.14472, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 91.94it/s]\n",
            "[50 / 50] Train: Loss = 0.06353, Accuracy = 97.74%: 100%|██████████| 572/572 [00:05<00:00, 111.36it/s]\n",
            "[50 / 50]   Val: Loss = 0.14827, Accuracy = 95.12%: 100%|██████████| 13/13 [00:00<00:00, 89.78it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ae4f4bd-0d90-4345-f0b6-9bcc00b22594"
      },
      "source": [
        "correct, total = 0, 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    cur_correct, cur_total = calc_accuracy(logits, y_batch)\n",
        "    correct += cur_correct\n",
        "    total += cur_total\n",
        "\n",
        "print(correct/total)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9515920803941212\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}